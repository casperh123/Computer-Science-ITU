{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5E0ztzgQ--cks3LYjQzjO",
      "metadata": {},
      "source": [
        "# Gaze estimation using Neural Networks\n",
        "## Data preparation\n",
        "This exercise focuses on using a multilayer perceptron (MLP) to estimate gaze using data collected in week 2 and processed in week 6 Data Preprocessing\n",
        ".\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "oAdmru4Vr_OD6Opr-wGHA",
      "metadata": {},
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "RNaQYoX-ta1mPa9pnCRVG",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 1 (easy): Load the data 2üë©‚Äçüíª**\n",
        "1. Run the cell below to load the pupil coordinates from the file `pupil_coordinates.csv`\n",
        " and screen coordinates in the file `screen_coordinates.csv`\n",
        " for the `grid`\n",
        " pattern of `test_subject_1`\n",
        ". The function `map_coordinates_to_targets`\n",
        " returns two $N \\times 2$ arrays containing labels (screen coordinates) corresponding to the inputs (pupil coordinates). \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "qD24G7PCpmMwOofOg6DSK",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import nn_util\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "test_subject = 'test_subject_1'\n",
        "session = '20250820_T0'\n",
        "pattern = 'grid'\n",
        "file_name_pupil = f'../W02/sessions/{test_subject}/{session}/{pattern}/frames/pupil_coordinates.csv'\n",
        "file_name_screen = f'../W02/sessions/{test_subject}/{session}/{pattern}/screen_coordinates.csv'\n",
        "\n",
        "input, target = nn_util.load_coordinates(file_name_pupil)\n",
        "labels = nn_util.map_targets_to_values(target, file_name_screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "SBfNmW5-CB9tziorYoEqp",
      "metadata": {},
      "source": [
        "The data set is divided into training and test data using train_test_split\n",
        " function from scikit-learn.\n",
        "\n",
        "---\n",
        "**Task 2 (easy): Prepare dataüë©‚Äçüíª**\n",
        "In the cell below:\n",
        "1. Use the function `train_test_split`\n",
        " to split the input and target data into a $80\\%/20\\%$ train/test sets.\n",
        "2. Use the function `train_test_split`\n",
        " to split the training into a $75\\%/25\\%$ train/validation sets.\n",
        "3. Visualize the data using the function `plot_data_splits`\n",
        " from the `nn_util.py`\n",
        " file.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "d8PCdrSvmTjj2yleeLNp8",
      "metadata": {},
      "source": [
        "# nn_util.plot_data_splits(X_train, X_val, X_test) # uncomment once the splits are made"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Y1ub8G3NRTd-Vfdp5Seep",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 3 (easy): Reflection on data splitüí°**\n",
        "1. Reflect on the benefits of making these splits and identify potential pitfals.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "aHPRZ3o7oJmF7z5b2MLBb",
      "metadata": {},
      "source": [
        "#Write your reflection here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Cs2cPQ9sGBWJrfp5BFxGI",
      "metadata": {},
      "source": [
        "## Linear Model\n",
        "In Assignment 1 Gaze Estimation\n",
        " Linear Least Square was used to find model parameters for a gaze estimation model.\n",
        "\n",
        "---\n",
        "**Task 4 (easy): Linear Least Squareüë©‚Äçüíª**\n",
        "1. Run the cell below to learn the model parameters using Linear Least Squares on the entire gaze data training set and visualize the result.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "IAbqNH32jE06RPnmLhLZF",
      "metadata": {},
      "source": [
        "pred_l = nn_util.least_square(X_train, Y_train, X_test)\n",
        "\n",
        "nn_util.plot_least_square_results(pred_l, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fC9sXvlg-EOo26UETUp7X",
      "metadata": {},
      "source": [
        "The cell below contains the definition of an affine model in Pytorch.\n",
        "\n",
        "---\n",
        "**Task 5 (easy): Train a linear model (gaze data)üë©‚Äçüíª**\n",
        "1. On a piece of paper draw the architecture of the network given the class definition `LinearModel`\n",
        ".\n",
        "**Note:** The class `MSELoss`\n",
        " explicitly defines the _Mean Squared Error_ loss function, for pedagogical reasons. Note, the Pytorch library has its own mse loss\n",
        ".\n",
        "\n",
        "\n",
        "2. Run the cell below to train the network.\n",
        "\n",
        "\n",
        "---\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "nSHE9H_0kSOok9DCBvDlP",
      "metadata": {},
      "source": [
        "class LinearModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dim (int): Number of input features.\n",
        "        output_dim (int): Number of output features.\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Passes the input through the linear layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
        " \n",
        "    def forward(self, x):\n",
        "        \"\"\"Args:\n",
        "        x (Tensor): Input tensor.\n",
        "        Returns:\n",
        "            Tensor: Output tensor after applying the linear transformation.\n",
        "        \"\"\"\n",
        "        x = self.linear1(x)\n",
        "        return x\n",
        "    \n",
        "class MSELoss(nn.Module):\n",
        "    def __init__(self, reduction='mean'):\n",
        "        super(MSELoss, self).__init__()\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        squared_diff = (input - target) ** 2\n",
        "        if self.reduction == 'mean':\n",
        "            return squared_diff.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return squared_diff.sum()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid reduction type. Use 'mean' or 'sum'.\")\n",
        "\n",
        "    \n",
        "def train_model(model, criterion, optimizer, X_train, Y_train, X_val=None, Y_val=None, num_epochs=100):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train.\n",
        "        criterion (nn.Module): The loss function to minimize.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
        "        X_train (Nx2 Tensor): Training input data.\n",
        "        Y_train (Nx2 Tensor): Training target data.\n",
        "        X_val (Nx2 Tensor, optional): Validation input data. Defaults to None.\n",
        "        Y_val (Nx2 Tensor, optional): Validation target data. Defaults to None.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "        list: Loss values for each epoch (training).\n",
        "        list: Loss values for each epoch (validation).\n",
        "        float: Training time.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    model_params = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, Y_train)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            model_params.append(model.parameters())\n",
        "            val_loss = criterion(val_outputs, Y_val)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    return train_losses, val_losses, training_time\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, X_test, Y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a trained model on test data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained neural network model.\n",
        "        X_test (Tensor): Test input data.\n",
        "        Y_test (Tensor): Test target data.\n",
        "\n",
        "    Returns:\n",
        "        float: Mean squared error (MSE) over the test set.\n",
        "        np.ndarray: Predicted values as a numpy array.\n",
        "        np.ndarray: True values as a numpy array.\n",
        "        np.ndarray: Absolute errors for x and y coordinates.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_output = model(X_test)\n",
        "        mse = mean_squared_error(Y_test.cpu().numpy(), test_output.cpu().numpy())\n",
        "        predictions = test_output.cpu().numpy()\n",
        "        true_values = Y_test.cpu().numpy()\n",
        "        errors = np.abs(true_values - predictions)\n",
        "\n",
        "    return mse, predictions, true_values, errors\n",
        "\n",
        "\n",
        "\n",
        "# Set parameters\n",
        "input_dim = 2\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "epoch = 20000\n",
        "\n",
        "# Generate training data\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)  \n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)  \n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)   \n",
        "\n",
        "model = LinearModel(input_dim, output_dim)\n",
        "criterion = MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "losses, val_losses, training_time = train_model(model, criterion, optimizer, X_train_tensor, Y_train_tensor, X_val_tensor, Y_val_tensor, num_epochs=epoch)\n",
        "\n",
        "# Test the model\n",
        "mse, Y_pred_lm, true_values, errors_nn = test_model(model, X_test_tensor, Y_test_tensor)\n",
        "print(f'Average MSE: {mse}')\n",
        "\n",
        "# Visualize results\n",
        "nn_util.plot_results(\n",
        "        Y_test_tensor,\n",
        "        Y_pred_lm,\n",
        "        errors_nn,\n",
        "        losses,\n",
        "        val_losses,\n",
        "        model_name='NN',\n",
        "        training_time=training_time\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "oojNx6B6n9QLFQ2A4nq5d",
      "metadata": {},
      "source": [
        "You will notice, that the neural network has a difficulty in predicting gaze compared to the linear least square optimization. \n",
        "\n",
        "---\n",
        "**Task 6 (easy): Analyse resultsüí°**\n",
        "1. Provide at least 3 reasons to why the neural network performs worse compared to the linear least squares. \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "0WdH6tkPgPxj5l1G1237V",
      "metadata": {},
      "source": [
        "# Write your reflections here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dMQVsFdpYT06UmBFItJq7",
      "metadata": {},
      "source": [
        "## Improving performance\n",
        "The following steps will investigate reasons for the poorer performance and include:\n",
        "- Data scale\n",
        "- The learning rate\n",
        "- The number of iterations\n",
        "\n",
        "### Data wrangling\n",
        "The following step investigate the impact of preprocessing of the data by normalizing the input and label data. It also investigates the impact of the learning rate and the number of iterations through `grid search`\n",
        ".\n",
        "\n",
        "**Data structure for plotting**\n",
        "To investigate the performance of the models the function `plot_results_collected`\n",
        " from the file `nn_util.py`\n",
        " is used. This function takes six dictionaries with models version as key containing the following for each model trained:\n",
        "- Model instance (object)\n",
        "- List of training losses (float)\n",
        "- List of validation losses (float)\n",
        "- Training time (float)\n",
        "- $N \\times 2$ array of predictions on test data (float)\n",
        "- List of prediction errors (float)\n",
        "\n",
        "The data needed to populate these data structures is provided gradually througout the exercise. It is important to name the models based on the data and hyper parameters used.\n",
        "\n",
        "\n",
        "---\n",
        "**Task 7 (easy): Train a linear model (normalized gaze data and synthetid gaze data)üë©‚Äçüíª**\n",
        "1. Complete the `DataScaler`\n",
        " class by implementing the `normalize`\n",
        " function and the `denormalize`\n",
        " function. \n",
        "\n",
        "\n",
        "$$\n",
        "x_{\\text{normalized}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "x_{\\text{denormalized}} = x_{\\text{normalized}} \\cdot (x_{\\max} - x_{\\min}) + x_{\\min}\n",
        "$$\n",
        "2. Use `DataScaler`\n",
        "to normalize the data in the `pupil_coordinates.csv`\n",
        " and `screen_coordinates.csv`\n",
        " files.\n",
        "3. The nested for-loops performs grid search of hyper parameter settings. In the nested for-loops:    - Train a model on the normalized data using the `train_model`\n",
        " function.\n",
        "    - Test the model using the `test_model`\n",
        " function.\n",
        "    - For each model store results in the designated dictionaries:        - Model instance (`LinearModel`\n",
        ")\n",
        "        - Loss (training)\n",
        "        - Loss (validation)\n",
        "        - Training time\n",
        "        - Predictions for test data \n",
        "        - Prediction errors \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Use the function `plot_results_collected`\n",
        " from the `nn_util.py`\n",
        " file, to visualize the result.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "Vto4YdW32_tPt245CvvCJ",
      "metadata": {},
      "source": [
        "class DataScaler:\n",
        "    def __init__(self):\n",
        "        self.min = None\n",
        "        self.max = None\n",
        "\n",
        "    def normalize(self, data):\n",
        "return normalized_data\n",
        "\n",
        "    def denormalize(self, normalized_data):\n",
        "return data\n",
        "    \n",
        "\n",
        "\n",
        "# Set hyperparameters\n",
        "input_dim = 2\n",
        "output_dim = 2\n",
        "learning_rate = [0.0001, 0.1, 1.5]\n",
        "epoch = [500, 2000, 10000]\n",
        "criterion = MSELoss()\n",
        "\n",
        "# Containers gaze data\n",
        "models_dict = {}\n",
        "losses_train_dict = {}\n",
        "losses_val_dict = {}\n",
        "training_time_dict = {}\n",
        "pred_norm_dict = {}\n",
        "errors_norm_dict = {}\n",
        "mse_norm_dict= {}\n",
        "\n",
        "\n",
        "#Train the models\n",
        "for i in learning_rate:\n",
        "    for j in epoch:\n",
        "\n",
        "\n",
        "\n",
        "nn_util.plot_mse_bar(mse_norm_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "WTZPmnXgqqxUSqgphRr00",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 8 (easy): Reflection on resultsüí°**\n",
        "1. What is the impact of normalizing the data and why?\n",
        "2. Experiment with the hyperparameter settings in the `learning_rate`\n",
        " and number of `epoch`\n",
        " lists.\n",
        "3. What are the benefits and cost of training with larger/smaller learning rate? Reflect on the effect of changing the learning rate.\n",
        "4. Reflect on the effect of the loss and training time when changing the number of epochs. \n",
        "5. What is the relationship between learning rate and epochs? \n",
        "6. The network architecture mimics the charateristics of a linear model, explain why the network architecture in many instances performs much different (and worse) compared to the linear least square?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "cKQ8k6ozwxl3NOXFj0J3b",
      "metadata": {},
      "source": [
        "# Write your reflections here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "svfs05j15STktdfCcuaI8",
      "metadata": {},
      "source": [
        "## Non-linear Model\n",
        "The following steps are about two different architectures for non-linear models. Compare the non-linear models to the affine model as done above. \n",
        "\n",
        "---\n",
        "**Task 9 (easy): Analyse architectureüí°**\n",
        "1. Examine the cell below to get an overview of the two model architectures and identify the main differences between the models.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "JuRCfizjwVusMGF0kkqdO",
      "metadata": {},
      "source": [
        "class NNRelu(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(NNRelu, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        " \n",
        "class NNRelu_exp(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(NNRelu_exp, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Hq6j6Cs932zqpu5-x0ypW",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "b3UdHPVH_f1BgI1rW1lXE",
      "metadata": {},
      "source": [
        "# Write your reflections here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "L9F7Gl9EyNhCP5xURdW3b",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 10 (easy): Train non-linear models (cleaned gaze data)üë©‚Äçüíª**\n",
        "**Note:** For the exam it may be convenient to copy the code from above to the cell below as you complete the steps. \n",
        "\n",
        "1. Rerun task [Task 7](#prediction100) using the normalized to train two new models. The steps are:\n",
        "\n",
        "- Train the models:\n",
        "    - Create two nested for-loops looping the lists containing hyperparameter values for `learning rate`\n",
        " and `epochs`\n",
        ". The loops should:        - Train models of both architectures on the cleaned normalized data, using the `train_model`\n",
        " function.\n",
        "        - Test the models using the `test_model`\n",
        " function.\n",
        "        - Save the following information in the designated dictionaries with the suffix `arc`\n",
        ", for each model:            - Model \n",
        "            - Loss (training)\n",
        "            - Loss (validation)\n",
        "            - Training time \n",
        "            - Predictions\n",
        "            - Errors \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Use the function `plot_results_collected`\n",
        " from the `nn_util.py`\n",
        " file, to visualize the result.\n",
        "\n",
        "- Use the function `plot_mse_bar`\n",
        " from the `nn_util.py`\n",
        " file, to visualize the mean squared error compared.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "ZZfup4jzwGYklOv_Pf1sM",
      "metadata": {},
      "source": [
        "# Set hyperparameters\n",
        "input_dim = 2\n",
        "output_dim = 2\n",
        "learning_rate = [0.001, 0.1, 1.5]\n",
        "epoch = [500, 2000, 10000]\n",
        "hidden_layer = 10\n",
        "\n",
        "criterion = MSELoss()\n",
        "\n",
        "\n",
        "models_dict_arc = {}\n",
        "losses_dict_arc = {}\n",
        "losses_val_dict_arc = {}\n",
        "training_time_dict_arc = {}\n",
        "pred_norm_arc = {}\n",
        "errors_norm_arc = {}\n",
        "mse_arc = {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in learning_rate:\n",
        "    for j in epoch:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VDNniw8TTjschL93Juhly",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 11 (easy): Reflect on the resultsüí°**\n",
        "1. Do more/less complex models improve the result? Why/why not?\n",
        "2. Are there indications of overfitting in any of the models?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "GR8nMJyzbtUeBUMCT5NE1",
      "metadata": {},
      "source": [
        "# Write your reflections here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "m5lH_WeqUpul_CUubvp1C",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 12 (easy): Analyse resultsüí°**\n",
        "1. Experiment with other architectures by suggesting models with different number layers and neurons in each layer.\n",
        "**Note:** This part of the exercise can easily become a timesink, mind your time as you proceed.\n",
        "\n",
        "\n",
        "2. Reflect on the results    - Do more/less complex models improve the result? Why/why not?\n",
        "    - Do any of the models show signs of overfitting?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "vroDDq98izQllGyZL24Mv",
      "metadata": {},
      "source": [
        "# Write your reflections here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "swYjz7vG24ZME1ab57iOB",
      "metadata": {},
      "source": [
        "## Own dataset\n",
        "Experiment with your own dataset and other models. \n",
        "**Note:** For the exam it may be convenient to copy your code from task [Task 7](#prediction100) to the cell below.\n",
        "This part of the exercise can easily become a timesink, mind your time as you proceed.\n",
        "\n",
        "\n",
        "---\n",
        "**Task 13 (easy): Train models (own dataset)üë©‚Äçüíªüí°**\n",
        "1. Experiment with your own dataset. \n",
        "2. Reflect on your results, compare to the results of `test_subject_1`\n",
        ". \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "2piauRe8hUeHGpXzuyIW4",
      "metadata": {},
      "source": [
        "# Write your reflections here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "rjXWRvEUdS_rTNQOT0tZn",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}